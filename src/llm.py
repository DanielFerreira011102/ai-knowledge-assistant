from typing import Iterator
from langchain_community.llms import Ollama


def create_llm(model_name: str, base_url: str, temperature: float) -> Ollama:
    """
    Initializes and configures the Language Model (LLM) client using the Ollama service.

    This function sets up the connection to the running Ollama instance, 
    specifying the target language model and important sampling parameters.

    For RAG applications, the temperature is intentionally kept very low 
    (typically between 0.0 and 0.2). This forces the model to be deterministic 
    and stick strictly to the facts provided in the retrieved context, 
    preventing creative or speculative answers (hallucination).

    :param model_name: The name of the LLM hosted by Ollama (e.g., "llama3").
    :type model_name: str
    :param base_url: The base URL of the local or remote Ollama server (e.g., "http://localhost:11434").
    :type base_url: str
    :param temperature: The sampling temperature to control the randomness of the output.
    :type temperature: float
    :return: The configured LangChain `Ollama` instance, ready for use.
    :rtype: Ollama
    """
    # The temperature is set here to ensure the LLM follows the RAG instructions 
    # and sticks to the provided context.
    return Ollama(
        model=model_name,
        base_url=base_url,
        temperature=temperature
    )


def generate_answer(llm: Ollama, prompt: str) -> str:
    """
    Generates a complete answer from the LLM using a synchronous (blocking) call.

    This function executes a simple synchronous call (`llm.invoke`), which 
    means the application waits until the LLM has finished generating the 
    entire response before returning the answer.

    :param llm: The configured LangChain `Ollama` instance.
    :type llm: Ollama
    :param prompt: The complete, structured RAG prompt string.
    :type prompt: str
    :return: The generated answer text, stripped of leading/trailing whitespace.
    :rtype: str
    """
    # LangChain's .invoke() executes the generation call synchronously.
    answer = llm.invoke(prompt)

    # We strip the output to clean up any potential extra newline characters 
    # the LLM might include at the start or end of its response.
    return answer.strip()


def generate_answer_stream(llm: Ollama, prompt: str) -> Iterator[str]:
    """
    Generates an answer from the LLM by streaming the output token-by-token.

    This method is highly recommended for user interfaces (like a web app 
    or an interactive console application) as it improves perceived latency
    by displaying the text as it is being generated, instead of waiting for the 
    entire response to finish.

    :param llm: The configured LangChain `Ollama` instance.
    :type llm: Ollama
    :param prompt: The complete, structured prompt string including context and question.
    :type prompt: str
    :return: Text chunks (tokens) as they are generated by the model.
    :rtype: Iterator[str]
    """
    # The .stream() method returns an iterator that yields tokens as they are generated.
    for chunk in llm.stream(prompt):
        yield chunk